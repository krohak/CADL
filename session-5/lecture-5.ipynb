{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5: Generative Models\n",
    "\n",
    "<p class=\"lead\">\n",
    "<a href=\"https://www.kadenze.com/courses/creative-applications-of-deep-learning-with-tensorflow/info\">Creative Applications of Deep Learning with Google's Tensorflow</a><br />\n",
    "<a href=\"http://pkmital.com\">Parag K. Mital</a><br />\n",
    "<a href=\"https://www.kadenze.com\">Kadenze, Inc.</a>\n",
    "</p>\n",
    "\n",
    "<a name=\"learning-goals\"></a>\n",
    "## Learning Goals\n",
    "\n",
    "\n",
    "<!-- MarkdownTOC autolink=true autoanchor=true bracket=round -->\n",
    "\n",
    "- [Introduction](#introduction)\n",
    "- [Generative Adversarial Networks](#generative-adversarial-networks)\n",
    "    - [Input Pipelines](#input-pipelines)\n",
    "    - [GAN/DCGAN](#gandcgan)\n",
    "    - [Extensions](#extensions)\n",
    "- [Recurrent Networks](#recurrent-networks)\n",
    "    - [Basic RNN Cell](#basic-rnn-cell)\n",
    "    - [LSTM RNN Cell](#lstm-rnn-cell)\n",
    "    - [GRU RNN Cell](#gru-rnn-cell)\n",
    "- [Character Langauge Model](#character-langauge-model)\n",
    "    - [Setting up the Data](#setting-up-the-data)\n",
    "    - [Creating the Model](#creating-the-model)\n",
    "    - [Loss](#loss)\n",
    "    - [Clipping the Gradient](#clipping-the-gradient)\n",
    "    - [Training](#training)\n",
    "    - [Extensions](#extensions-1)\n",
    "- [DRAW Network](#draw-network)\n",
    "- [Future](#future)\n",
    "- [Homework](#homework)\n",
    "- [Examples](#examples)\n",
    "- [Reading](#reading)\n",
    "\n",
    "<!-- /MarkdownTOC -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First check the Python version\n",
    "import sys\n",
    "if sys.version_info < (3,4):\n",
    "    print('You are running an older version of Python!\\n\\n',\n",
    "          'You should consider updating to Python 3.4.0 or',\n",
    "          'higher as the libraries built for this course',\n",
    "          'have only been tested in Python 3.4 and higher.\\n')\n",
    "    print('Try installing the Python 3.5 version of anaconda'\n",
    "          'and then restart `jupyter notebook`:\\n',\n",
    "          'https://www.continuum.io/downloads\\n\\n')\n",
    "\n",
    "# Now get necessary libraries\n",
    "try:\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from skimage.transform import resize\n",
    "    from skimage import data\n",
    "    from scipy.misc import imresize\n",
    "    from scipy.ndimage.filters import gaussian_filter\n",
    "    import IPython.display as ipyd\n",
    "    import tensorflow as tf\n",
    "    from libs import utils, gif, datasets, dataset_utils, nb_utils\n",
    "except ImportError as e:\n",
    "    print(\"Make sure you have started notebook in the same directory\",\n",
    "          \"as the provided zip file which includes the 'libs' folder\",\n",
    "          \"and the file 'utils.py' inside of it.  You will NOT be able\",\n",
    "          \"to complete this assignment unless you restart jupyter\",\n",
    "          \"notebook inside the directory created by extracting\",\n",
    "          \"the zip file or cloning the github repo.\")\n",
    "    print(e)\n",
    "\n",
    "# We'll tell matplotlib to inline any drawn figures like so:\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .rendered_html code { \n",
       "    padding: 2px 4px;\n",
       "    color: #c7254e;\n",
       "    background-color: #f9f2f4;\n",
       "    border-radius: 4px;\n",
       "} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bit of formatting because I don't like the default inline code style:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style> .rendered_html code { \n",
    "    padding: 2px 4px;\n",
    "    color: #c7254e;\n",
    "    background-color: #f9f2f4;\n",
    "    border-radius: 4px;\n",
    "} </style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"introduction\"></a>\n",
    "# Introduction\n",
    "\n",
    "So far we've seen the basics of neural networks, how they can be used for encoding large datasets, or for predicting labels.  We've also seen how to interrogate the deeper representations that networks learn in order to help with their objective, and how amplifying some of these objectives led to creating deep dream.  Finally, we saw how the representations in deep nets trained on object recognition are capable of representing both style and content, and how we could independently manipulate a new image to have the style of one image, and the content of another.\n",
    "\n",
    "In this session we'll start to explore some more generative models.  We've already seen how an autoencoder is composed of both an encoder which takes an input and represents it into some hidden state vector.  From this hidden state vector, a decoder is capable of resynthesizing the original input, though with some loss.  So think back to the decoders that we've already built.  It has an internal state, and from that state, it can express the entire distribution of the original data, that is, it can express any possible image that is has seen.\n",
    "\n",
    "We call that a generative model as it is capable of generating the distribution of the data.  Contrast this to the latter half of Session 3 when we saw how we label an image using supervised learning.  This model is really trying to discriminate the data distribution based on the extra labels that we have.  So this is another helpful distinction with machine learning algorithms, ones that are generative and others that are discriminative.\n",
    "\n",
    "In this session, we'll explore more generative models, and states can be used to generate data in two other very powerful generative networks, one based on game theory called the generative adversarial network, and another capable of remembering and forgetting over time, allowing us to model dynamic content and sequences, called the recurrent neural network.\n",
    "\n",
    "<a name=\"generative-adversarial-networks\"></a>\n",
    "# Generative Adversarial Networks\n",
    "\n",
    "In session 3, we were briefly introduced to the Variational Autoencoder.  This network was very powerful because it encompasses a very strong idea.  And that idea is measuring distance not necessarily based on pixels, but in some \"semantic space\".  And I mentioned then that we'd see another type of network capable of generating even better images of CelebNet.\n",
    "\n",
    "So this is where we're heading...\n",
    "\n",
    "We're now going to see how to do that using what's called the generative adversarial network.\n",
    "\n",
    "The generative adversarial network is actually two networks.  One called the generator, and another called the discriminator.  The basic idea is the generator is trying to create things which look like the training data.  So for images, more images that look like the training data.  The discriminator has to guess whether what its given is a real training example.  Or whether its the output of the generator.  By training one after another, you ensure neither are ever too strong, but both grow stronger together.  The discriminator is also learning a distance function!  This is pretty cool because we no longer need to measure pixel-based distance, but we learn the distance function entirely!\n",
    "\n",
    "The Generative Adversarial Network, or GAN, for short, are in a way, very similar to the autoencoder we created in session 3.  Or at least the implementation of it is.  The discriminator is a lot like the encoder part of this network, except instead of going down to the 64 dimensions we used in our autoencoder, we'll reduce our input down to a single value, yes or no, 0 or 1, denoting yes its a true training example, or no, it's a generated one.\n",
    "\n",
    "And the generator network is exactly like the decoder of the autoencoder.  Except, there is nothing feeding into this inner layer.  It is just on its own.  From whatever vector of hidden values it starts off with, it will generate a new example meant to look just like the training data.  One pitfall of this model is there is no explicit encoding of an input.  Meaning, you can't take an input and find what would possibly generate it.  However, there are recent extensions to this model which make it more like the autoencoder framework, allowing it to do this.\n",
    "\n",
    "<a name=\"input-pipelines\"></a>\n",
    "## Input Pipelines\n",
    "\n",
    "Before we get started, we're going to need to work with a very large image dataset, the CelebNet dataset.  In session 1, we loaded this dataset but only grabbed the first 1000 images.  That's because loading all 200 thousand images would take up a lot of memory which we'd rather not have to do.  And in Session 3 we were introduced again to the CelebNet and Sita Sings the Blues which required us to load a lot of images.  I glossed over the details of the input pipeline then so we could focus on learning the basics of neural networks.  But I think now we're ready to see how to handle some larger datasets.\n",
    "\n",
    "Tensorflow provides operations for taking a list of files, using that list to load the data pointed to it, decoding that file's data as an image, and creating shuffled minibatches.  All of this is put into a queue and managed by queuerunners and coordinators.\n",
    "\n",
    "As you may have already seen in the Variational Autoencoder's code, I've provided a simple interface for creating such an input pipeline using image files which will also apply cropping and reshaping of images in the pipeline so you don't have to deal with any of it.  Let's see how we can use it to load the CelebNet dataset.\n",
    "\n",
    "\n",
    "Let's first get the list of all the CelebNet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from libs.datasets import CELEB\n",
    "files = CELEB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create our input pipeline to create shuffled minibatches and crop the images to a standard shape.  This will require us to specify the list of files, how large each minibatch is, how many epochs we want to run for, and how we want the images to be cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from libs.dataset_utils import create_input_pipeline\n",
    "batch_size = 100\n",
    "n_epochs = 10\n",
    "input_shape = [218, 178, 3]\n",
    "crop_shape = [64, 64, 3]\n",
    "crop_factor = 0.8\n",
    "batch = create_input_pipeline(\n",
    "    files=files,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    "    crop_shape=crop_shape,\n",
    "    crop_factor=crop_factor,\n",
    "    shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then when we are ready to use the batch generator, we'll need to create a `Coordinator` and specify this to tensorflow using the `start_queue_runners` method in order to provide the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab our data using our `batch` generator like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 64, 64, 3)\n",
      "float32 255.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5f6c7e940>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb5f7723550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_xs = sess.run(batch)\n",
    "# We get batch_size at a time, so 100\n",
    "print(batch_xs.shape)\n",
    "# The datatype is float32 since what is what we use in the tensorflow graph\n",
    "# And the max value still has the original image range from 0-255\n",
    "print(batch_xs.dtype, np.max(batch_xs))\n",
    "# So to plot it, we'll need to divide by 255.\n",
    "plt.imshow(batch_xs[0] / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to make use of this while we train a generative adversarial network!\n",
    "\n",
    "<a name=\"gandcgan\"></a>\n",
    "## GAN/DCGAN\n",
    "\n",
    "Inside the libs directory, you'll find `gan.py` which shows how to create a generative adversarial network with or without convolution, and how to train it using the CelebNet dataset.  Let's step through the code and then I'll show you what it's capable of doing.\n",
    "\n",
    "-- Code demonstration not transcribed. -- \n",
    "\n",
    "<a name=\"extensions\"></a>\n",
    "## Extensions\n",
    "\n",
    "So it turns out there are a ton of very fun and interesting extensions when you have a model in this space.  It turns out that you can perform addition in the latent space.  I'll just show you Alec Radford's code base on github to show you what that looks like.\n",
    "\n",
    "<a name=\"recurrent-networks\"></a>\n",
    "# Recurrent Networks\n",
    "\n",
    "Up until now, all of the networks that we've learned and worked with really have no sense of time.  They are static.  They cannot remember sequences, nor can they understand order outside of the spatial dimensions we offer it.  Imagine for instance that we wanted a network capable of reading.  As input, it is given one letter at a time.  So let's say it were given the letters 'n', 'e', 't', 'w', 'o', 'r', and we wanted it to learn to output 'k'.   It would need to be able to reason about inputs it received before the last one it received, the letters before 'r'.  But it's not just letters.\n",
    "\n",
    "Consider the way we look at the world.  We don't simply download a high resolution image of the world in front of us.  We move our eyes.  Each fixation takes in new information and each of these together in sequence help us perceive and act.  That again is a sequential process.\n",
    "\n",
    "Recurrent neural networks let us reason about information over multiple timesteps.  They are able to encode what it has seen in the past as if it has a memory of its own.  It does this by basically creating one HUGE network that expands over time.  It can reason about the current timestep by conditioning on what it has already seen.  By giving it many sequences as batches, it can learn a distribution over sequences which can model the current timestep given the previous timesteps.  But in order for this to be practical, we specify at each timestep, or each time it views an input, that the weights in each new timestep cannot change.  We also include a new matrix, `H`, which reasons about the past timestep, connecting each new timestep.  For this reason, we can just think of recurrent networks as ones with loops in it.\n",
    "\n",
    "Other than that, they are exactly like every other network we've come across!  They will have an input and an output.  They'll need a loss or an objective function to optimize which will relate what we want the network to output for some given set of inputs.  And they'll be trained with gradient descent and backprop.\n",
    "\n",
    "<a name=\"basic-rnn-cell\"></a>\n",
    "## Basic RNN Cell\n",
    "\n",
    "The basic recurrent cell can be used in tensorflow as `tf.contrib.rnn.BasicRNNCell`.  Though for most complex sequences, especially longer sequences, this is almost never a good idea.  That is because the basic RNN cell does not do very well as time goes on.  To understand why this is, we'll have to learn a bit more about how backprop works.  When we perform backrprop, we're multiplying gradients from the output back to the input.  As the network gets deeper, there are more multiplications along the way from the output to the input.\n",
    "\n",
    "Same for recurrent networks.  Remember, their just like a normal feedforward network with each new timestep creating a new layer.  So if we're creating an infinitely deep network, what will happen to all our multiplications?  Well if the derivatives are all greater than 1, then they will very quickly grow to infinity.  And if they are less than 1, then they will very quickly grow to 0. That makes them very difficult to train in practice.  The problem is known in the literature as the exploding or vanishing gradient problem.  Luckily, we don't have to figure out how to solve it, because some very clever people have already come up with a solution, in 1997!, yea, what were you doing in 1997.  Probably not coming up with they called the long-short-term-memory, or LSTM.\n",
    "\n",
    "<a name=\"lstm-rnn-cell\"></a>\n",
    "## LSTM RNN Cell\n",
    "\n",
    "The mechanics of this are unforunately far beyond the scope of this course, but put simply, it uses a combinations of gating cells to control its contents and by having gates, it is able to block the flow of the gradient, avoiding too many multiplications during backprop.  For more details, I highly recommend reading: https://colah.github.io/posts/2015-08-Understanding-LSTMs/.\n",
    "\n",
    "In tensorflow, we can make use of this cell using `tf.contrib.rnn.LSTMCell`.\n",
    "\n",
    "<a name=\"gru-rnn-cell\"></a>\n",
    "## GRU RNN Cell\n",
    "\n",
    "One last cell type is worth mentioning, the gated recurrent unit, or GRU.  Again, beyond the scope of this class.  Just think of it as a simplifed version of the LSTM with 2 gates instead of 4, though that is not an accurate description.  In Tensorflow we can use this with `tf.contrib.rnn.GRUCell`.\n",
    "\n",
    "<a name=\"character-langauge-model\"></a>\n",
    "# Character Langauge Model\n",
    "\n",
    "We'll now try a fun application of recurrent networks where we try to model a corpus of text, one character at a time.  The basic idea is to take one character at a time and try to predict the next character in sequence.  Given enough sequences, the model is capable of generating entirely new sequences all on its own.\n",
    "\n",
    "<a name=\"setting-up-the-data\"></a>\n",
    "## Setting up the Data\n",
    "\n",
    "For data, we're going to start with text.  You can basically take any text file that is sufficiently long, as we'll need a lot of it, and try to use this.  This website seems like an interesting place to begin: http://textfiles.com/directory.html and project guttenberg https://www.gutenberg.org/browse/scores/top.  http://prize.hutter1.net/ also has a 50k euro reward for compressing wikipedia.  Let's try w/ Alice's Adventures in Wonderland by Lewis Carroll:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "import ssl\n",
    "\n",
    "with open('alice.txt', 'r') as fp:\n",
    "    txt = fp.read()\n",
    "\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's find out what's inside this text file by creating a set of all possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163810, 84)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list(set(txt))\n",
    "len(txt), len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great so we now have about 164 thousand characters and 85 unique characters in our vocabulary which we can use to help us train a model of language.  Rather than use the characters, we'll convert each character to a unique integer.  We'll later see that when we work with words, we can achieve a similar goal using a very popular model called word2vec: https://www.tensorflow.org/versions/r0.9/tutorials/word2vec/index.html\n",
    "\n",
    "We'll first create a look up table which will map a character to an integer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = dict(zip(vocab, range(len(vocab))))\n",
    "decoder = dict(zip(range(len(vocab)), vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"creating-the-model\"></a>\n",
    "## Creating the Model\n",
    "\n",
    "For our model, we'll need to define a few parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sequences in a mini batch\n",
    "batch_size = 100\n",
    "\n",
    "# Number of characters in a sequence\n",
    "sequence_length = 100\n",
    "\n",
    "# Number of cells in our LSTM layer\n",
    "n_cells = 256\n",
    "\n",
    "# Number of LSTM layers\n",
    "n_layers = 2\n",
    "\n",
    "# Total number of characters in the one-hot encoding\n",
    "n_chars = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the input and output to the network.  Rather than having `batch size` x `number of features`; or `batch size` x `height` x `width` x `channels`; we're going to have `batch size` x `sequence length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, sequence_length], name='X')\n",
    "\n",
    "# We'll have a placeholder for our true outputs\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length], name='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now remember with MNIST that we used a one-hot vector representation of our numbers.  We could transform our input data into such a representation.  But instead, we'll use `tf.nn.embedding_lookup` so that we don't need to compute the encoded vector.  Let's see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 100, 256]\n"
     ]
    }
   ],
   "source": [
    "# we first create a variable to take us from our one-hot representation to our LSTM cells\n",
    "embedding = tf.get_variable(\"embedding\", [n_chars, n_cells])\n",
    "\n",
    "# And then use tensorflow's embedding lookup to look up the ids in X\n",
    "Xs = tf.nn.embedding_lookup(embedding, X)\n",
    "\n",
    "# The resulting lookups are concatenated into a dense tensor\n",
    "print(Xs.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a recurrent network, we're going to need to slice our sequences into individual inputs.  That will give us timestep lists which are each `batch_size` x `input_size`.  Each character will then be connected to a recurrent layer composed of `n_cells` LSTM units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's create a name scope for the operations to clean things up in our graph\n",
    "with tf.name_scope('reslice'):\n",
    "    Xs = [tf.squeeze(seq, [1])\n",
    "          for seq in tf.split(Xs, sequence_length, axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our recurrent layer composed of LSTM cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cells = tf.contrib.rnn.BasicLSTMCell(num_units=n_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll initialize our LSTMs using the convenience method provided by tensorflow.  We could explicitly define the batch size here or use the `tf.shape` method to compute it based on whatever `X` is, letting us feed in different sizes into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now we have a layer of recurrent cells and a way to initialize them.  If we wanted to make this a multi-layer recurrent network, we could use the `MultiRNNCell` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build deeper recurrent net if using more than 1 layer\n",
    "if n_layers > 1:\n",
    "    cells = [cells]\n",
    "    for layer_i in range(1, n_layers):\n",
    "        with tf.variable_scope('{}'.format(layer_i)):\n",
    "            this_cell = tf.contrib.rnn.BasicLSTMCell(\n",
    "                num_units=n_cells)\n",
    "            cells.append(this_cell)\n",
    "    cells = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    initial_state = cells.zero_state(tf.shape(X)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In either case, the cells are composed of their outputs as modulated by the LSTM's output gate, and whatever is currently stored in its memory contents.  Now let's connect our input to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this will return us a list of outputs of every element in our sequence.\n",
    "# Each output is `batch_size` x `n_cells` of output.\n",
    "# It will also return the state as a tuple of the n_cells's memory and\n",
    "# their output to connect to the time we use the recurrent layer.\n",
    "outputs, state = tf.contrib.rnn.static_rnn(cells, Xs, initial_state=initial_state)\n",
    "\n",
    "# We'll now stack all our outputs for every cell\n",
    "outputs_flat = tf.reshape(tf.concat(outputs, axis=1), [-1, n_cells])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our output, we'll simply try to predict the very next timestep.  So if our input sequence was \"networ\", our output sequence should be: \"etwork\".  This will give us the same batch size coming out, and the same number of elements as our input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('prediction'):\n",
    "    W = tf.get_variable(\n",
    "        \"W\",\n",
    "        shape=[n_cells, n_chars],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "    b = tf.get_variable(\n",
    "        \"b\",\n",
    "        shape=[n_chars],\n",
    "        initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "\n",
    "    # Find the output prediction of every single character in our minibatch\n",
    "    # we denote the pre-activation prediction, logits.\n",
    "    logits = tf.matmul(outputs_flat, W) + b\n",
    "\n",
    "    # We get the probabilistic version by calculating the softmax of this\n",
    "    probs = tf.nn.softmax(logits)\n",
    "\n",
    "    # And then we can find the index of maximum probability\n",
    "    Y_pred = tf.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"loss\"></a>\n",
    "## Loss\n",
    "\n",
    "Our loss function will take the reshaped predictions and targets, and compute the softmax cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('loss'):\n",
    "    # Compute mean cross entropy loss for each output.\n",
    "    Y_true_flat = tf.reshape(tf.concat(Y, axis=1), [-1])\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=Y_true_flat)\n",
    "    mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"clipping-the-gradient\"></a>\n",
    "## Clipping the Gradient\n",
    "\n",
    "Normally, we would just create an optimizer, give it a learning rate, and tell it to minize our loss.  But with recurrent networks, we can help out a bit by telling it to clip gradients.  That helps with the exploding gradient problem, ensureing they can't get any bigger than the value we tell it.  We can do that in tensorflow by iterating over every gradient and variable, and changing their value before we apply their update to every trainable variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    gradients = []\n",
    "    clip = tf.constant(5.0, name=\"clip\")\n",
    "    for grad, var in optimizer.compute_gradients(mean_loss):\n",
    "        gradients.append((tf.clip_by_value(grad, -clip, clip), var))\n",
    "    updates = optimizer.apply_gradients(gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also explore other methods of clipping the gradient based on a percentile of the norm of activations or other similar methods, like when we explored deep dream regularization.  But the LSTM has been built to help regularize the network through its own gating mechanisms, so this may not be the best idea for your problem.  Really, the only way to know is to try different approaches and see how it effects the output on your problem.\n",
    "\n",
    "<a name=\"training\"></a>\n",
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.47325\n",
      "['', '', '', '', '', '', '', '', '', '', '', '777777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '777', '', '', '777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7777', '', '', '', '', '', '', '', '7', '', '', '', '77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '77777', '', '', '', '', '', '', '', '', '', '', '', '', '', '7777', '7777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '77777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '77777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '777777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '77', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '7777', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4.43501\n",
      "2 4.3481\n",
      "3 4.09374\n",
      "4 3.96246\n",
      "5 3.66755\n",
      "6 3.58873\n",
      "7 3.54601\n",
      "8 3.47566\n",
      "9 3.35612\n",
      "10 3.37087\n",
      "11 3.35737\n",
      "12 3.28991\n",
      "13 3.30449\n",
      "14 3.37734\n",
      "15 3.48912\n",
      "16 3.34242\n",
      "17 3.20425\n",
      "18 3.2195\n",
      "19 3.17281\n",
      "20 3.20068\n",
      "21 3.22115\n",
      "22 3.18901\n",
      "23 3.20404\n",
      "24 3.25338\n",
      "25 3.1841\n",
      "26 3.17565\n",
      "27 3.26047\n",
      "28 3.23435\n",
      "29 3.21951\n",
      "30 3.25726\n",
      "31 3.3283\n",
      "32 3.48172\n",
      "33 3.1491\n",
      "34 3.17049\n",
      "35 3.15593\n",
      "36 3.19415\n",
      "37 3.17481\n",
      "38 3.13431\n",
      "39 3.15235\n",
      "40 3.20871\n",
      "41 3.17822\n",
      "42 3.12877\n",
      "43 3.21699\n",
      "44 3.23332\n",
      "45 3.15345\n",
      "46 3.21971\n",
      "47 3.27747\n",
      "48 3.42454\n",
      "49 3.19846\n",
      "50 3.11816\n",
      "51 3.14505\n",
      "52 3.11464\n",
      "53 3.15466\n",
      "54 3.1296\n",
      "55 3.09541\n",
      "56 3.15786\n",
      "57 3.14111\n",
      "58 3.08197\n",
      "59 3.12969\n",
      "60 3.183\n",
      "61 3.11035\n",
      "62 3.12736\n",
      "63 3.20026\n",
      "64 3.33013\n",
      "65 3.2251\n",
      "66 3.04705\n",
      "67 3.03125\n",
      "68 3.04114\n",
      "69 3.0313\n",
      "70 3.04\n",
      "71 2.97206\n",
      "72 2.98116\n",
      "73 3.01611\n",
      "74 2.93024\n",
      "75 2.91712\n",
      "76 2.99674\n",
      "77 2.97549\n",
      "78 2.87784\n",
      "79 2.94647\n",
      "80 3.10537\n",
      "81 3.26147\n",
      "82 2.8244\n",
      "83 2.8471\n",
      "84 2.89504\n",
      "85 2.82367\n",
      "86 2.82409\n",
      "87 2.75912\n",
      "88 2.7532\n",
      "89 2.77477\n",
      "90 2.73847\n",
      "91 2.66117\n",
      "92 2.75498\n",
      "93 2.7561\n",
      "94 2.69753\n",
      "95 2.73826\n",
      "96 2.85532\n",
      "97 3.09923\n",
      "98 2.77507\n",
      "99 2.72103\n",
      "100 2.63465\n",
      "101 2.71037\n",
      "102 2.64787\n",
      "103 2.68351\n",
      "104 2.59432\n",
      "105 2.59564\n",
      "106 2.61674\n",
      "107 2.5384\n",
      "108 2.55836\n",
      "109 2.63463\n",
      "110 2.60853\n",
      "111 2.55719\n",
      "112 2.70583\n",
      "113 2.9358\n",
      "114 2.84088\n",
      "115 2.59364\n",
      "116 2.54485\n",
      "117 2.61298\n",
      "118 2.54123\n",
      "119 2.58808\n",
      "120 2.50668\n",
      "121 2.50865\n",
      "122 2.5153\n",
      "123 2.48584\n",
      "124 2.44794\n",
      "125 2.53687\n",
      "126 2.54514\n",
      "127 2.43977\n",
      "128 2.49434\n",
      "129 2.78539\n",
      "130 2.90625\n",
      "131 2.52263\n",
      "132 2.5036\n",
      "133 2.54103\n",
      "134 2.47667\n",
      "135 2.50182\n",
      "136 2.46411\n",
      "137 2.44721\n",
      "138 2.43346\n",
      "139 2.42268\n",
      "140 2.3891\n",
      "141 2.44203\n",
      "142 2.44697\n",
      "143 2.41969\n",
      "144 2.43038\n",
      "145 2.5659\n",
      "146 2.80874\n",
      "147 2.61176\n",
      "148 2.43595\n",
      "149 2.4134\n",
      "150 2.42951\n",
      "151 2.40556\n",
      "152 2.439\n",
      "153 2.37742\n",
      "154 2.3541\n",
      "155 2.38303\n",
      "156 2.34397\n",
      "157 2.32861\n",
      "158 2.39698\n",
      "159 2.39914\n",
      "160 2.31811\n",
      "161 2.43369\n",
      "162 2.69634\n",
      "163 2.69316\n",
      "164 2.36756\n",
      "165 2.35019\n",
      "166 2.3875\n",
      "167 2.3734\n",
      "168 2.34285\n",
      "169 2.31133\n",
      "170 2.31374\n",
      "171 2.31212\n",
      "172 2.31453\n",
      "173 2.26169\n",
      "174 2.32664\n",
      "175 2.34795\n",
      "176 2.23258\n",
      "177 2.30213\n",
      "178 2.55514\n",
      "179 2.73553\n",
      "180 2.39421\n",
      "181 2.29171\n",
      "182 2.28884\n",
      "183 2.31354\n",
      "184 2.30808\n",
      "185 2.28096\n",
      "186 2.25944\n",
      "187 2.24947\n",
      "188 2.23861\n",
      "189 2.22241\n",
      "190 2.23156\n",
      "191 2.26638\n",
      "192 2.24355\n",
      "193 2.22703\n",
      "194 2.38379\n",
      "195 2.60744\n",
      "196 2.53646\n",
      "197 2.23591\n",
      "198 2.2586\n",
      "199 2.22019\n",
      "200 2.25276\n",
      "201 2.22155\n",
      "202 2.21946\n",
      "203 2.18129\n",
      "204 2.19198\n",
      "205 2.20109\n",
      "206 2.14991\n",
      "207 2.20886\n",
      "208 2.22421\n",
      "209 2.13677\n",
      "210 2.22546\n",
      "211 2.50849\n",
      "212 2.63225\n",
      "213 2.20226\n",
      "214 2.2122\n",
      "215 2.18748\n",
      "216 2.25075\n",
      "217 2.16931\n",
      "218 2.15818\n",
      "219 2.1444\n",
      "220 2.13983\n",
      "221 2.14682\n",
      "222 2.11702\n",
      "223 2.1367\n",
      "224 2.15409\n",
      "225 2.11524\n",
      "226 2.15203\n",
      "227 2.35495\n",
      "228 2.57829\n",
      "229 2.29631\n",
      "230 2.16516\n",
      "231 2.15954\n",
      "232 2.13722\n",
      "233 2.17149\n",
      "234 2.12347\n",
      "235 2.11852\n",
      "236 2.0681\n",
      "237 2.10745\n",
      "238 2.06752\n",
      "239 2.07058\n",
      "240 2.11511\n",
      "241 2.11646\n",
      "242 2.06071\n",
      "243 2.22951\n",
      "244 2.44486\n",
      "245 2.43327\n",
      "246 2.10556\n",
      "247 2.13178\n",
      "248 2.10269\n",
      "249 2.15013\n",
      "250 2.07157\n",
      "251 2.08735\n",
      "252 2.0457\n",
      "253 2.03872\n",
      "254 2.0672\n",
      "255 2.01843\n",
      "256 2.06233\n",
      "257 2.08603\n",
      "258 2.00942\n",
      "259 2.05487\n",
      "260 2.36966\n",
      "261 2.53298\n",
      "262 2.08701\n",
      "263 2.0921\n",
      "264 2.05446\n",
      "265 2.13238\n",
      "266 2.06513\n",
      "267 2.03381\n",
      "268 2.02819\n",
      "269 1.9945\n",
      "270 2.00876\n",
      "271 1.99447\n",
      "272 2.0027\n",
      "273 2.01551\n",
      "274 2.01359\n",
      "275 2.02492\n",
      "276 2.18674\n",
      "277 2.41295\n",
      "278 2.24016\n",
      "279 2.05492\n",
      "280 2.05129\n",
      "281 2.01495\n",
      "282 2.06567\n",
      "283 2.00217\n",
      "284 2.00986\n",
      "285 1.93795\n",
      "286 1.9847\n",
      "287 1.96592\n",
      "288 1.93306\n",
      "289 1.98982\n",
      "290 2.01475\n",
      "291 1.94207\n",
      "292 2.08547\n",
      "293 2.28724\n",
      "294 2.34541\n",
      "295 2.01358\n",
      "296 2.02336\n",
      "297 1.99882\n",
      "298 2.05803\n",
      "299 1.96271\n",
      "300 1.96746\n",
      "301 1.93853\n",
      "302 1.91838\n",
      "303 1.96481\n",
      "304 1.90051\n",
      "305 1.93651\n",
      "306 1.98255\n",
      "307 1.88814\n",
      "308 1.94513\n",
      "309 2.20525\n",
      "310 2.37126\n",
      "311 2.06186\n",
      "312 1.97126\n",
      "313 1.97972\n",
      "314 1.9885\n",
      "315 1.98588\n",
      "316 1.94032\n",
      "317 1.93509\n",
      "318 1.87465\n",
      "319 1.91084\n",
      "320 1.89106\n",
      "321 1.87153\n",
      "322 1.91351\n",
      "323 1.91626\n",
      "324 1.90559\n",
      "325 2.05282\n",
      "326 2.23524\n",
      "327 2.2212\n",
      "328 1.94457\n",
      "329 1.96731\n",
      "330 1.91365\n",
      "331 1.96642\n",
      "332 1.89826\n",
      "333 1.92876\n",
      "334 1.84319\n",
      "335 1.86972\n",
      "336 1.87847\n",
      "337 1.83042\n",
      "338 1.88056\n",
      "339 1.90771\n",
      "340 1.84416\n",
      "341 1.93805\n",
      "342 2.14322\n",
      "343 2.28743\n",
      "344 1.90759\n",
      "345 1.94753\n",
      "346 1.91701\n",
      "347 1.96164\n",
      "348 1.86623\n",
      "349 1.87951\n",
      "350 1.84532\n",
      "351 1.83001\n",
      "352 1.86002\n",
      "353 1.8154\n",
      "354 1.82403\n",
      "355 1.86034\n",
      "356 1.80109\n",
      "357 1.86342\n",
      "358 2.03484\n",
      "359 2.24718\n",
      "360 2.0055\n",
      "361 1.88394\n",
      "362 1.9099\n",
      "363 1.86587\n",
      "364 1.89336\n",
      "365 1.83865\n",
      "366 1.86157\n",
      "367 1.77924\n",
      "368 1.82919\n",
      "369 1.78639\n",
      "370 1.77407\n",
      "371 1.81356\n",
      "372 1.83363\n",
      "373 1.79524\n",
      "374 1.94768\n",
      "375 2.10121\n",
      "376 2.1424\n",
      "377 1.84171\n",
      "378 1.88648\n",
      "379 1.8602\n",
      "380 1.88128\n",
      "381 1.8023\n",
      "382 1.83258\n",
      "383 1.77598\n",
      "384 1.76559\n",
      "385 1.81886\n",
      "386 1.7371\n",
      "387 1.78331\n",
      "388 1.80769\n",
      "389 1.75473\n",
      "390 1.80307\n",
      "391 2.02079\n",
      "392 2.21579\n",
      "393 1.84923\n",
      "394 1.83832\n",
      "395 1.83359\n",
      "396 1.8872\n",
      "397 1.80485\n",
      "398 1.78625\n",
      "399 1.77183\n",
      "400 1.7442\n",
      "401 1.7692\n",
      "402 1.73984\n",
      "403 1.73038\n",
      "404 1.74928\n",
      "405 1.75574\n",
      "406 1.78073\n",
      "407 1.90106\n",
      "408 2.10897\n",
      "409 1.96471\n",
      "410 1.82145\n",
      "411 1.84025\n",
      "412 1.78818\n",
      "413 1.82107\n",
      "414 1.75685\n",
      "415 1.78865\n",
      "416 1.68706\n",
      "417 1.7549\n",
      "418 1.7208\n",
      "419 1.693\n",
      "420 1.72115\n",
      "421 1.76101\n",
      "422 1.71445\n",
      "423 1.84866\n",
      "424 1.95954\n",
      "425 2.06175\n",
      "426 1.78789\n",
      "427 1.8106\n",
      "428 1.78662\n",
      "429 1.81463\n",
      "430 1.71967\n",
      "431 1.75009\n",
      "432 1.71271\n",
      "433 1.69058\n",
      "434 1.73979\n",
      "435 1.6715\n",
      "436 1.68212\n",
      "437 1.73781\n",
      "438 1.66668\n",
      "439 1.73457\n",
      "440 1.89447\n",
      "441 2.08947\n",
      "442 1.82394\n",
      "443 1.75973\n",
      "444 1.77836\n",
      "445 1.77026\n",
      "446 1.75092\n",
      "447 1.71447\n",
      "448 1.72428\n",
      "449 1.65966\n",
      "450 1.69761\n",
      "451 1.66077\n",
      "452 1.65663\n",
      "453 1.67091\n",
      "454 1.68281\n",
      "455 1.70277\n",
      "456 1.79777\n",
      "457 1.94236\n",
      "458 1.97012\n",
      "459 1.74239\n",
      "460 1.77842\n",
      "461 1.7084\n",
      "462 1.75261\n",
      "463 1.67637\n",
      "464 1.72744\n",
      "465 1.62825\n",
      "466 1.66434\n",
      "467 1.66597\n",
      "468 1.62436\n",
      "469 1.65318\n",
      "470 1.6915\n",
      "471 1.63192\n",
      "472 1.75107\n",
      "473 1.83365\n",
      "474 2.01757\n",
      "475 1.72889\n",
      "476 1.74459\n",
      "477 1.72872\n",
      "478 1.75403\n",
      "479 1.6612\n",
      "480 1.68416\n",
      "481 1.65233\n",
      "482 1.613\n",
      "483 1.67669\n",
      "484 1.61575\n",
      "485 1.63006\n",
      "486 1.64604\n",
      "487 1.59942\n",
      "488 1.67005\n",
      "489 1.78437\n",
      "490 1.99371\n",
      "491 1.79867\n",
      "492 1.69639\n",
      "493 1.72461\n",
      "494 1.68395\n",
      "495 1.6835\n",
      "496 1.66023\n",
      "497 1.67247\n",
      "498 1.58733\n",
      "499 1.63475\n",
      "500 1.59971\n",
      "[\"teow '\", 'said tlice  ahrelietd tn ter sfn tomltle ', \"'I  s aotaettng   af taTc \", '', '', \"'he sueen thrned tooee   tith torn  and  an er tradeng an ter sor t corent oote tnlatl te n   ahaoaded tIf  thth ter serd \", 'tf  to', '', \"'Io 'e ted' said tlice  aery siosey and toatned y  and the sueen aas thme t  \", \"'he sing tord tes wedd tp n aer hnee and thmeney aoid tIoueeoe   au tear  the st tfey tnloosd '\", '', \"'he sueen thrned t d eny tnay toom tes  and thid th hhe wione aahrneahe  wfer '\", '', \"'he sione sod th 'aery sore  r y  aath tf  torr \", '', \"'Ie  tn ' said the Mueen  an a lheeele aoos teice  and the sheee satdeneds on eent y tusnen tp  and tegan tetnng th the wing  ahe wueen  ahe weo n aoand edt and tvery ou  txoed\", '', \"'Ietre tf  thet '\", 'saeiaded the wueen ', \"'Iou kane te oone  \", '', \"Ald the   thrneng th hhe weot,-hoed ahe want on  aIhat taTE Iou ce n towng ter  ' \", \"'Ios tn aaaase tou  harect    said the  at a lery hernee th e  aonng town tn tfe tnow tn tersaooed aIhnwane the ng -'\", '', \"'I wha ' said the Mueen  aho sad ta rde nl te   txplengng toe weot, \", '', \"Ih  thth the r sard  ' snd the saook s nn aarer tf  aheee wf the saoeend  ae ann ng te ing th txe ttidtoe wpdortsnttidootted ds  aha set th slice wor taomedt on \", '', \"'Iou keoldt tertegi d d '\", 'said tlice  and the sar ooe  st o t lirge toownds-ortahet shauk aoar ', \"'he sheee waoeende aasd d d tnout tor tncongtedof tha  aiokeng tor the   and the  suetn e aadkh d tf  tn er the srhere \", '', \"'Ind the r sard  tf  ' saeug d the Mueen \", '', \"'Ihe r sard  tne tone  an tt aaaase tou  harect  '\", 'she saoeend  theul d tn aeali ', '', \"'Ihet s seght '\", 'saeug d the Mueen ', \"'Iotdtou caas aoovuete'\", '', \"'he saoe n   tare thdl t  and tooked tn tlice  an the sueet nn ais tvene t y aaan  oor aer \", '', \"'Ior ' saeug d tlece \", '', \"'Ioue tn  ahe  ' seudd  the sueen  and tlice auungd the waome,t nn  ahrder ng tery sash aaat tantd tedper toat \", '', \"'I  s -at s a lery sond ton '\", 'said t lhleneteice,tn ter same ', \"'he was tasleng ae the shite sabbit  ahe sas tarseng tndinns y an o ter hork \", '', \"'Iore   said tlice  'I -hane s the worhe   ' \", \"'Iost 'Iott ' said the Mabbit ht a lion aerdend to e \", \"'erwooked tndeons y afer tes hoeuld d tn terwaooed and the  setded tes  df tp n thmelns aor aes sauneeaoase ao ter sxds and taat ered tIoe s tpder thet dte of txelttion \", '', '', \"'Ihat sor ' said tlice \", '', \"'Ioneaou cei tThit t lrtt ' \", '', 'she satbit h  ed ', '', \"'Iow a sonn't   said tlice  'I won't thenk tt s a  t l tslrtt \", \"'tshid tThit tor ' \", '', \"'Ihe wett  the sueen s sadee-t\", ' he sabbit he an ', \"'lice sate t little shooadeof tirtet r \", \"'If, aaste' she satbit taat ered tn a loonht ned to e \", \"'Ihe sueen tatl terr tou 'Tou kee  ahe wate tet er sirt  and the suenn thid -'\", '', \"'Iec th sou  aaase  '\", 'saeug d the Mueen an a leice,af thetg ds and tartee tegan tesdin  tnout tt a l tosestion   ahrneeng ap a ain.  aarh afher  aeu ner  ahe  woo ahe ee  town tn a longth tf tha  and the srte tegan ', \"'lice sheught the wad toaer the  thrh a lorlous ooomuete-rowsd on aer titt  't sas t l teg en and tortins  ahe setl  sare tite ter entn   ahe wadl d  aike toereng ns  and the saoeend  aad th te nte the   dfed tp a d th teend tf the r sadd  and tort  ah sene the wnehiss\", \"  he soosd tos enely  alice aornd tn tontt aas tt aangteng aer hoeceng n 'he sarhedd d tn art ing tnh tete thrhed tnay  aoueirtiile axtrght ander terssnee ahth tt  aiat teddeng town  aut tottnelly  aut  tn the wad too tn  aoat oote y aoaendet d d tft  and tas tonng to tete the war en n tnceiwntath tt  terr  at shULd this  it  lf,temnd tnd tiokeop at aer hore  ahth thrh a lrrlied tntlaas nn ahet she sauld tot aerl tete  ng tft tirthtng  and taan the wad too an  aerr town  and tas tonng to te angtnein  at sas tery saoweweng ah tend thet she war en n tar tpdeulyd tn  lf  and tas tt ahe wne of taon yng tnay  au tne  tnl then  ahe e was sottnelyy snlegeentf tortinson ahe was oaane er the wasder th thetethe tar en n th  and  an the soonle  -n aomeend  aire tnl y  oot eng tp and tasleng af  th tfhe  sarse af the wrowsd  alice saon aore th the touteaseon ahet st was t lery hosuenely oote tn erd \", '', \"'he saas r  tnl taased tn tfee oath ut iasteng tor thrne  auiteeslyng tnl the waone  and tonht ng tor the war en n   and tt a lery hoeue thme the wueen aas tt a lortnns oarteon  and tant ohoneeng tneut  and theul ng taf  thth tes werr ' sn tIf  thth ter serd '\", 'snlut tf e an a congte ', '', \"'lice segan th tee  tery spd r   'h se ahre  ahe wad tot an tor oed t d rostlredtith the wueen  aut the snow then tt washt tedper tnd sasgted aInd the    sheught the  sIhat sauld te ouentf te 'Ihe  se soead erly  or etf te irr ng tarpee tar   ahe wooat oarder tt  ahet she e s tnd sfe tiat tnlce  \", '', \"'he sas tooketg t out tor thme ths tn txeeied and tander ng tian er the sauld not onay aath ut ie ng toem  ahan t e tot ne  tnlorlons onpertette on ahe wnd  at sarlled ter sery sash an tonst  aut  an er ths h ng at snlongte tf tha  ahe wade tn aft oh te anlroeg and the said th her elf ta  s ahe waaatene tat  totat shall tede teoe ere th thik ah  \", '', \"'Iew tne tou cot eng tf ' said the Mate an th n t  the e was tant  txerghttor at ah teeetitith \", '', \"'lice sasded thml the wxer onperted  and the  soteen \", \"'I  s aotsse sheateng to tt   sae sheught  aIhml tt  avts aade touen af an tiase of  tf the  \", '', 'A  t dther tangte she waone terr t perted  and the  slice aartoonn aer soeceng n and tegan tndtnkeusdeof the wrte  ao   ng tery soan the wad th e n dth tittld ah ter ', \"'he sat the ed to theng ohet she e was svowghtaf tt tot tn ahght  and totsone tf tt a perted \", '', \"'I won't thenk the  waasean t l torr y   slice segan  atgtetter sscoupeetn ng ao e  aInd the  sll tueteeslah tioad elly af  torst oerrdtfe  df toeati-and the  sow't the eth tede t d sesld tn aarsenullr   n tiase  an the e wne  aoweue tn er   ah the  -and tou de totst dnteu tomtoreog tn an a l the woeng  aegng tnlce  aor tt eeite  ahe e s she sneh s me sen th te theeughttoat oasleng anout t  the wfher sxgetf the trowsd -and t shauld teve toomueted the tueen s sar entn tust aot  an y tn aesga ay aaan tt aai tang tomeng ' \", \"'Iew to sou koke the wueen ' thid the Mateat a lionaeice \", '', \"'Iot an t l   said tlice, 'Ihe s th sxt inedy -'\", ' ust the  the wot ne  thet she sueen tas toase aegang ter  aottld ng  ah the sant on  aI -ose y th tatd ahet st s aadd y thrk  taite tongtteng the wrte ', '', '', \"'he sueen thenl  tnd tarted tf \", '', \"'Iha slE Iou chlk ng to ' said the sing  ao ng tp ah tlice  and tookeng tn the wat s aarr thth troat ioreon oi \", '', \"'I  s a loond  tf tetg -'nlaaa ene tat   said tlice, 'Inl w ta oh bt eiwete an \", '', '', \"'I son't toke the wookeof tt a  t l   thid the Ming  'Iew ner  at sad tnnt aa tedd tn tt aate  \", '', '', \"'I l tetter sot   she sat teaard d \", '', \"'Ionet te an oreeng t   said the Ming  aand ton't tioket  te site thet '\", '', 'arson ae ing tlice an tersaooe ', '', \"'Incan aad tiokeo  t lnng   said tlice,\", \"'I ve sead thet st ahme tett   ut t son't teaeneer tiane \", '', '', \"'Ihll  at aast te aeained   said the Ming oery soatned y  and tersorled the wueen  ahi sas tart tg t  the warent  aIo woar 'I sash tou canld beve then sor aeaaned '\", '', \"'he sueen tad tfey tf  ths tn the eeng tnl tosuenely nn  aooat tf ahenle\", '', \"Ih  thth tes tard ' sae said  ahth ut txer tiokeng teund \", '', \"'I ll tee h ahe wxt ttion d ta elf   said the Ming oarhn y  and tersasdend tf  \", '', \"'lice sheught the sanht tn tarl ao tetk  and the teu the wrte oas tonng tf  an the sardd the wueen s seice an ahe wit  nte  ahaiadeng tith trrteon \", \"'he wad t l dd  terrd ter haetedte ooeee wf the waased  oo te tneltth  tor tedeng tant d toe r shrne  and the sod tot aite the wookeof theng  an t l  an the sote oas tt thlh aomtereon ahet she woaer tnow than er tt was tar shrneaf aot \", \"'h she sant on ahet h af ter her entn \", '', \"'he sar entn tas txoered tn a lonht aath t dther ter entn  ahith ahe ed to tlice andaxphsfyd  tf ertirtni ior aaomueteng af  tf the  wath the sfher  ahe sfee sod enely  ais  ahet ser soedeng nths tone onkeot ao the tfher shme tf the wrtsen  ahine slice aauld the tn aheing tn a larl y   thmeatf ths th toe tp an o t lhee \", '', \"'u the shme the wad tontht aoe wooreng rt d teewsht at aetk  ahe sorhe tas tner  and teth ahe sar entn  aare tft tf thght  aIet tt sond  t seneer aash   sheught tlice  a\", 'n t l the wneh ssane tote toom then weme tf the wrowsd ', '', 'Ah she shrh d tn a ay apder ter hnee ahet tt aasht tot tatele tnain. and tant oetk aor t little tare touterselion aith ter soondd ', '', \"'hin the wen tetk ah the waaatene tat  ahe sas threeined to tond tuete a cirge toomieionledt d teund tn  ahe e sas t lonteredtonng tf tegtird the wxpsttion d  ahe wing  and the sueen  aho sare t l thikeng an tf e  'hete t l the weat oare tuete ahdl t  and tioked tery spdeueirtiile \", '', \"'he sauent olice anperted  ahe was t perryd to te t l theee wh see ee the wueet nn  and the  seaented the r s eeredt  ao ter  theught an the  wll theoedtn tnee  ahe sornd tt aery sedd tn erd th teke tft taplt y wiat the  waid \", '', \"'he sxe tteon d t tneeredt ois  ahet sou could 't sor an  t lerr tpde s the e was t lete to tor onhtf  teem  thet sersad toaer ted th te shlh a cheng tegore  and tersas 't totng to te angan taT shme sf titt \", '', \"'he sing s sneeredt ois  ahet s d hing thet sed t lerr tould oe tegirr d  and thet sou ca  dtt th shlk tot  rt d\", '', \"'he sueen s sneeredt ois  ahet st thme hing tis  t sowg tnout tt at aiat thendtotshme the s tede txery ou  axpctted  anl teund \", \"'I  sas then sirt beaenkeohet sed tane the waone sarse aiokeoh trone and t dious \", '', '', \"'lice sauld theng tf tot ing taoedto tei tet tI  wegang  ah the sorhess  'ou d tegter t  eta  wnout tt '\", '', \"'Ihe s at aaoetne  she sueen thid th hhe wxpsttion d  'Io  hiaar seas \", '', '', 'ld the sxe tteon d thrt on  tite tndtneiuh', '', \"' he sat s aa r te an toreng tnay ahe wanent oerwas oote  and  au the shme terwad touenoetk aath the worhess  an wad txo ne y tostleedted  a  the wing tnd the wxt ttion d tetcahtl e ap and town tiokeng tor at  ahate the weat of the warse aa   oetk ah the wrte \", '', \"'   oATTER A   The Mouk Turtle s sooue w Tou kan't mheng teu toat a wnaah tee tou cnain  aou koar tfl theng '\", '', 'aid the Mochess  an the shrh d ter sneet  ertion nidy an o tlice s ']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501 1.58581\n",
      "502 1.60743\n",
      "503 1.63854\n",
      "504 1.6134\n",
      "505 1.73297\n",
      "506 1.85168\n",
      "507 1.91212\n",
      "508 1.65965\n",
      "509 1.73056\n",
      "510 1.66631\n",
      "511 1.6952\n",
      "512 1.60696\n",
      "513 1.66328\n",
      "514 1.57776\n",
      "515 1.58564\n",
      "516 1.6328\n",
      "517 1.56075\n",
      "518 1.59895\n",
      "519 1.61314\n",
      "520 1.56452\n",
      "521 1.66432\n",
      "522 1.74084\n",
      "523 1.98397\n",
      "524 1.67329\n",
      "525 1.67204\n",
      "526 1.67103\n",
      "527 1.69809\n",
      "528 1.60587\n",
      "529 1.61565\n",
      "530 1.59929\n",
      "531 1.55608\n",
      "532 1.59915\n",
      "533 1.5757\n",
      "534 1.55021\n",
      "535 1.56668\n",
      "536 1.56758\n",
      "537 1.60917\n",
      "538 1.69261\n",
      "539 1.89364\n",
      "540 1.75783\n",
      "541 1.6561\n",
      "542 1.68611\n",
      "543 1.61209\n",
      "544 1.63469\n",
      "545 1.57965\n",
      "546 1.62567\n",
      "547 1.52363\n",
      "548 1.58677\n",
      "549 1.5349\n",
      "550 1.53881\n",
      "551 1.5494\n",
      "552 1.58664\n",
      "553 1.53559\n",
      "554 1.6786\n",
      "555 1.75189\n",
      "556 1.84764\n",
      "557 1.62344\n",
      "558 1.65293\n",
      "559 1.6322\n",
      "560 1.63598\n",
      "561 1.55332\n",
      "562 1.59226\n",
      "563 1.5493\n",
      "564 1.51637\n",
      "565 1.58119\n",
      "566 1.51572\n",
      "567 1.53122\n",
      "568 1.55911\n",
      "569 1.50745\n",
      "570 1.57853\n",
      "571 1.68031\n",
      "572 1.90179\n",
      "573 1.64086\n",
      "574 1.62409\n",
      "575 1.6138\n",
      "576 1.63099\n",
      "577 1.56934\n",
      "578 1.55432\n",
      "579 1.56508\n",
      "580 1.49905\n",
      "581 1.54522\n",
      "582 1.50585\n",
      "583 1.51495\n",
      "584 1.50175\n",
      "585 1.52263\n",
      "586 1.55395\n",
      "587 1.61403\n",
      "588 1.76181\n",
      "589 1.767\n",
      "590 1.6046\n",
      "591 1.63298\n",
      "592 1.55778\n",
      "593 1.59537\n",
      "594 1.51618\n",
      "595 1.57475\n",
      "596 1.46655\n",
      "597 1.52511\n",
      "598 1.50709\n",
      "599 1.47913\n",
      "600 1.50766\n",
      "601 1.53207\n",
      "602 1.46847\n",
      "603 1.62105\n",
      "604 1.6433\n",
      "605 1.8082\n",
      "606 1.58345\n",
      "607 1.60484\n",
      "608 1.5889\n",
      "609 1.59112\n",
      "610 1.5035\n",
      "611 1.53486\n",
      "612 1.5094\n",
      "613 1.45958\n",
      "614 1.53617\n",
      "615 1.46297\n",
      "616 1.49558\n",
      "617 1.5024\n",
      "618 1.44167\n",
      "619 1.52408\n",
      "620 1.60408\n",
      "621 1.81151\n",
      "622 1.64263\n",
      "623 1.56371\n",
      "624 1.58521\n",
      "625 1.54971\n",
      "626 1.52885\n",
      "627 1.51065\n",
      "628 1.52841\n",
      "629 1.44685\n",
      "630 1.49404\n",
      "631 1.45422\n",
      "632 1.45457\n",
      "633 1.469\n",
      "634 1.4931\n",
      "635 1.46609\n",
      "636 1.56356\n",
      "637 1.68221\n",
      "638 1.74253\n",
      "639 1.5362\n",
      "640 1.59841\n",
      "641 1.52894\n",
      "642 1.55028\n",
      "643 1.46558\n",
      "644 1.53462\n",
      "645 1.43117\n",
      "646 1.45242\n",
      "647 1.48896\n",
      "648 1.43281\n",
      "649 1.46854\n",
      "650 1.47263\n",
      "651 1.42266\n",
      "652 1.5333\n",
      "653 1.55161\n",
      "654 1.80226\n",
      "655 1.54259\n",
      "656 1.56017\n",
      "657 1.54276\n",
      "658 1.55512\n",
      "659 1.46608\n",
      "660 1.4838\n",
      "661 1.4655\n",
      "662 1.42042\n",
      "663 1.47533\n",
      "664 1.43736\n",
      "665 1.43719\n",
      "666 1.44467\n",
      "667 1.40833\n",
      "668 1.48086\n",
      "669 1.52598\n",
      "670 1.72921\n",
      "671 1.61269\n",
      "672 1.53149\n",
      "673 1.56986\n",
      "674 1.48384\n",
      "675 1.5004\n",
      "676 1.45361\n",
      "677 1.50361\n",
      "678 1.39313\n",
      "679 1.4595\n",
      "680 1.4051\n",
      "681 1.42251\n",
      "682 1.42862\n",
      "683 1.44996\n",
      "684 1.39258\n",
      "685 1.53505\n",
      "686 1.59983\n",
      "687 1.686\n",
      "688 1.49626\n",
      "689 1.54567\n",
      "690 1.50946\n",
      "691 1.51267\n",
      "692 1.42753\n",
      "693 1.46305\n",
      "694 1.41701\n",
      "695 1.4001\n",
      "696 1.4578\n",
      "697 1.40138\n",
      "698 1.41621\n",
      "699 1.43216\n",
      "700 1.37721\n",
      "701 1.46161\n",
      "702 1.51132\n",
      "703 1.73564\n",
      "704 1.51825\n",
      "705 1.5097\n",
      "706 1.50298\n",
      "707 1.52859\n",
      "708 1.45101\n",
      "709 1.42978\n",
      "710 1.4384\n",
      "711 1.3757\n",
      "712 1.43195\n",
      "713 1.39284\n",
      "714 1.40739\n",
      "715 1.38365\n",
      "716 1.38573\n",
      "717 1.43308\n",
      "718 1.47268\n",
      "719 1.62897\n",
      "720 1.59311\n",
      "721 1.50018\n",
      "722 1.51713\n",
      "723 1.44966\n",
      "724 1.47489\n",
      "725 1.40029\n",
      "726 1.45509\n",
      "727 1.34483\n",
      "728 1.41698\n",
      "729 1.38041\n",
      "730 1.36699\n",
      "731 1.39097\n",
      "732 1.40135\n",
      "733 1.35181\n",
      "734 1.49842\n",
      "735 1.4851\n",
      "736 1.65086\n",
      "737 1.47789\n",
      "738 1.4946\n",
      "739 1.47082\n",
      "740 1.48263\n",
      "741 1.39634\n",
      "742 1.40727\n",
      "743 1.39258\n",
      "744 1.34702\n",
      "745 1.42308\n",
      "746 1.35551\n",
      "747 1.37362\n",
      "748 1.38795\n",
      "749 1.32536\n",
      "750 1.41626\n",
      "751 1.45125\n",
      "752 1.65887\n",
      "753 1.51599\n",
      "754 1.46215\n",
      "755 1.48271\n",
      "756 1.44555\n",
      "757 1.42157\n",
      "758 1.40617\n",
      "759 1.41659\n",
      "760 1.32515\n",
      "761 1.3872\n",
      "762 1.35238\n",
      "763 1.35201\n",
      "764 1.36039\n",
      "765 1.35094\n",
      "766 1.36675\n",
      "767 1.44047\n",
      "768 1.52366\n",
      "769 1.59715\n",
      "770 1.44728\n",
      "771 1.49404\n",
      "772 1.4103\n",
      "773 1.44506\n",
      "774 1.36162\n",
      "775 1.4191\n",
      "776 1.33279\n",
      "777 1.35335\n",
      "778 1.35955\n",
      "779 1.32711\n",
      "780 1.36223\n",
      "781 1.35953\n",
      "782 1.31193\n",
      "783 1.43354\n",
      "784 1.41454\n",
      "785 1.63236\n",
      "786 1.42988\n",
      "787 1.46427\n",
      "788 1.43629\n",
      "789 1.44884\n",
      "790 1.35634\n",
      "791 1.37229\n",
      "792 1.36069\n",
      "793 1.30704\n",
      "794 1.37962\n",
      "795 1.31824\n",
      "796 1.33863\n",
      "797 1.32998\n",
      "798 1.28317\n",
      "799 1.3795\n",
      "800 1.3906\n",
      "801 1.59301\n",
      "802 1.49136\n",
      "803 1.41503\n",
      "804 1.45497\n",
      "805 1.37536\n",
      "806 1.39156\n",
      "807 1.35429\n",
      "808 1.39152\n",
      "809 1.29102\n",
      "810 1.34823\n",
      "811 1.30505\n",
      "812 1.31139\n",
      "813 1.31045\n",
      "814 1.33015\n",
      "815 1.29369\n",
      "816 1.4103\n",
      "817 1.44894\n",
      "818 1.56238\n",
      "819 1.3905\n",
      "820 1.45144\n",
      "821 1.38989\n",
      "822 1.41047\n",
      "823 1.31773\n",
      "824 1.3602\n",
      "825 1.30465\n",
      "826 1.29805\n",
      "827 1.35755\n",
      "828 1.29101\n",
      "829 1.32536\n",
      "830 1.30816\n",
      "831 1.27276\n",
      "832 1.36344\n",
      "833 1.35137\n",
      "834 1.59119\n",
      "835 1.42141\n",
      "836 1.41609\n",
      "837 1.40058\n",
      "838 1.41788\n",
      "839 1.34291\n",
      "840 1.32935\n",
      "841 1.3223\n",
      "842 1.28045\n",
      "843 1.3276\n",
      "844 1.29645\n",
      "845 1.30185\n",
      "846 1.29214\n",
      "847 1.27113\n",
      "848 1.32446\n",
      "849 1.34267\n",
      "850 1.49282\n",
      "851 1.46996\n",
      "852 1.40135\n",
      "853 1.43538\n",
      "854 1.35158\n",
      "855 1.37044\n",
      "856 1.30661\n",
      "857 1.3606\n",
      "858 1.24677\n",
      "859 1.32112\n",
      "860 1.27245\n",
      "861 1.29195\n",
      "862 1.29151\n",
      "863 1.29494\n",
      "864 1.24665\n",
      "865 1.39034\n",
      "866 1.36013\n",
      "867 1.50778\n",
      "868 1.37889\n",
      "869 1.4104\n",
      "870 1.37739\n",
      "871 1.3881\n",
      "872 1.28604\n",
      "873 1.32439\n",
      "874 1.29634\n",
      "875 1.26283\n",
      "876 1.32745\n",
      "877 1.26416\n",
      "878 1.28099\n",
      "879 1.29477\n",
      "880 1.23495\n",
      "881 1.31993\n",
      "882 1.31577\n",
      "883 1.51796\n",
      "884 1.40703\n",
      "885 1.36831\n",
      "886 1.38351\n",
      "887 1.36578\n",
      "888 1.3309\n",
      "889 1.29397\n",
      "890 1.31296\n",
      "891 1.2452\n",
      "892 1.30791\n",
      "893 1.24828\n",
      "894 1.27111\n",
      "895 1.2541\n",
      "896 1.25754\n",
      "897 1.28137\n",
      "898 1.31805\n",
      "899 1.38387\n",
      "900 1.48265\n",
      "901 1.36626\n",
      "902 1.39479\n",
      "903 1.31816\n",
      "904 1.35905\n",
      "905 1.26717\n",
      "906 1.32236\n",
      "907 1.22835\n",
      "908 1.27353\n",
      "909 1.26969\n",
      "910 1.2539\n",
      "911 1.25415\n",
      "912 1.27264\n",
      "913 1.21242\n",
      "914 1.35728\n",
      "915 1.27767\n",
      "916 1.47804\n",
      "917 1.35719\n",
      "918 1.38116\n",
      "919 1.35044\n",
      "920 1.35861\n",
      "921 1.26705\n",
      "922 1.28862\n",
      "923 1.27659\n",
      "924 1.22044\n",
      "925 1.30174\n",
      "926 1.231\n",
      "927 1.26627\n",
      "928 1.23822\n",
      "929 1.2023\n",
      "930 1.28486\n",
      "931 1.27837\n",
      "932 1.46078\n",
      "933 1.39655\n",
      "934 1.33333\n",
      "935 1.36702\n",
      "936 1.3118\n",
      "937 1.29641\n",
      "938 1.28245\n",
      "939 1.29846\n",
      "940 1.20528\n",
      "941 1.27794\n",
      "942 1.22116\n",
      "943 1.24738\n",
      "944 1.23141\n",
      "945 1.24949\n",
      "946 1.22442\n",
      "947 1.30019\n",
      "948 1.33633\n",
      "949 1.43977\n",
      "950 1.31493\n",
      "951 1.38354\n",
      "952 1.32179\n",
      "953 1.33967\n",
      "954 1.23402\n",
      "955 1.30216\n",
      "956 1.2233\n",
      "957 1.22339\n",
      "958 1.28279\n",
      "959 1.21471\n",
      "960 1.24927\n",
      "961 1.24464\n",
      "962 1.18803\n",
      "963 1.31285\n",
      "964 1.2528\n",
      "965 1.47265\n",
      "966 1.34195\n",
      "967 1.34333\n",
      "968 1.32324\n",
      "969 1.36035\n",
      "970 1.2626\n",
      "971 1.2539\n",
      "972 1.25854\n",
      "973 1.2116\n",
      "974 1.27029\n",
      "975 1.22742\n",
      "976 1.22491\n",
      "977 1.22222\n",
      "978 1.19957\n",
      "979 1.25214\n",
      "980 1.24808\n",
      "981 1.4197\n",
      "982 1.3899\n",
      "983 1.32947\n",
      "984 1.35212\n",
      "985 1.27854\n",
      "986 1.29979\n",
      "987 1.24609\n",
      "988 1.28417\n",
      "989 1.17754\n",
      "990 1.26653\n",
      "991 1.20695\n",
      "992 1.21513\n",
      "993 1.19512\n",
      "994 1.22367\n",
      "995 1.17517\n",
      "996 1.31342\n",
      "997 1.27664\n",
      "998 1.39744\n",
      "999 1.31724\n",
      "1000 1.35854\n",
      "['', \"h  Ihe pochess  wha wuchess   h, Hou't wao we toyete in t ve gnet aar hasting ' slice relt th soace  li ohit she was noadi to b k ters of tny ofe  'o  shon the Mabbit wame toar oer  ahe wegan  an a liw  ahmidlooice  'a  you coaase  yhde-'\", '', 'he cabbit waatk d tollently  aoawped the woite Rid toaves and ahe cir   nd sheceied onay an o the tonk  r, on sedd as se sould go  ', \"'lice whuk tp ahe tir ond d awes  and  as she wadl ras gery lau  whe wnnt srl ing aor elf tll the crme the want on ohlking  tIo r  ioar  Iow duier ilerything ws to -ey,'Tnd yottird b whings aart on tust is ssecl  ' conder wt y \", \"e segn aaanged tn ahe desht 'Iet se shisg  whs t chi rame than t wot tp thes aoreing 'I snl st toin  o can aemember teel ng asrittle wosferent   ut It I v aot aoi thme  the pext wuistion os  thi ss ahe sorkd wnoo  In  wHAT S ahe wreat wrtzle ' sld the wegan soeng ng afer ttl the coild en whe wnee thet iore wf the thme tsa al se  elf  ah see tt the would nave teen aoanged tor s y of the   \", \"'T m aone i m aot al c  sae said  'Ior ter hadd hrts on tuch aiog aagghe,s  and sanu tonsn't bo nn aengle,s an hll  'nd t m nure t han t te aarbl  yor t snow wnl ta t  of thisgs  and she  au  Ihe wnew  woch tncery sittle  Tu tde   ahE   she  and t m n  tnd -'f dear! tow tutlling at wll tn 'I ml sai tn y cn t wnl the tring  a csed to bnow  Tet me seem torr hhme  aore yn too le  and torn ahme  ahd on toaseh n  and torn shme  sheer sn -'f dear  I dhall bover sot io thint  an thet wote   owever, the socl neeeetion oHsle sows 't saghen   tot s aheitucpeonh , \", \"owgin as toi sotetil of trrcsi wnd srrct on toe sotltil of teue, tnd tauentaot whAT S all thiwg  a m aartainl'I cust bave been woange  tor aarbl  t ml chi t d thy aTow do h the wittle -''\", 'hnd she woidted ter hasds of ter fiseon st ahe wane thiing aiasens  and segan so semeat tt   ut te  soice aoonder tewrse,ind shaenge  and she cork  wod not coue oha stme ts she  wse  to bo  -', '', \"      Iow ye   the tittle doowkkene        nplooi tes dhelgng thkn        nd waos oha soyer  wf the sotec       n aleny looden sartl '       Iaw doeareully faasaeme th toeng        Iw aeariy saoead Iis eoers        nd yinlese wittle sort   an        hll arte y shalicg aucs  \", '', \"'W m aore thiue wle oo  aoi toght ward    said tlor Alice, and ta  haes worl d oith thace anain.in she cant on  'a wust be sarbl Tnter tll  asd t dhall bave bo bo tny dike at thet waoe iithee sause, and sade covt th sotso e oh sray thth  and sn  Tverysemsaky toasons oo teart  Ao  w me ran  ap ay tisd onout it  an y l aarbl  t vl gaat town tere 'I  sl te ao rse ooi r sat eng thi r saads aown atd shi ng tTome op anain, aoar   c sholl bf y miok ap and dhy aTha ss I toi  'Ihll ye ohit donet  wnd she   an y cake te ng aoat sarsens w ll came tp  tt yoti'w ml saat town tere thml y'm ao e euy slse  -'ut  yn dear   sried Alice  'hth a lucden wrtit wf thars  aI wonnith the  whULD iur ihe reaaad  oown 'I wn Io tERY lumed of cecng tll tllne tor  ' \", \"'l she said toes aio wooked aown at ter setd   and sas aocprise  to hae  hat she wad nlt on tne of the tabbit s roktle giite Rid tlaves aiace tha was nhlking  'Wow dOT a'have yone ohat ' sae shought  'I wust be srow ng ooalleosain,'\", \"Ahe sot ts and dirt oo the trble wo taaneresserself fy tt  and sound ahat  as soarly ss she could nots   the sas aot tnout iho ciel oish  and tas aoing tn toeilging aothtiy  'he waon wornd ant ohat she Qotle tf thes aos ooi fir ohe was sald ng  and she woewped rn wavtily  aust an thme th s enc theilg ng alan ollorether  \", \"'That wAT t largew ovcepe ' said tlice. 'nlrod doar tooehtened tt ohe sorden ooange  tet iery load th tind ter elf fhall in axcctarce  'Ind tow wor the srrden ' snd she sen sath t l thr r ayck io the tottle soor  \", \"ut  aslr  Ihe cittle goor aas aoer wsain, and she cottle golden sey wis teing tf the trass tokle ws segore  aInd thasks ase nhuke that tver   shought Ahe Moor oaildr 'Ior t sever sas ah shill ws shes aogore  aover 'Tld t doaeine st s ao  mec  yhet is ws ' \", \"'l she said toe   wouds aarefort thakped  and tn a  ther forent  aheaiei  he was np ao ter sail sn auykeoiser.\", \"'oreainst anea tos ahat she had nole or wollen an o the toa- tWnd tt ahet wore w han ao aeck iu tetdlay   sae said to herself  'Ilice wad neen ao hhe toacene of e on ter hite, and sad none oo the trttralitonfeedeon  ahat soine er sou coodo sn the sngrash ionlt iou cond t letblr of tecieng tockengs on the stam ah etooild en wosnlng tn ahe somc oith thrd r iheke   aha  slleunrf tioeeng towse,  and seging the  atdebd ap ooateons  Hewever, the waon wake out ohat she was nn the dror of thlrs iiich whe wet tireeahar the was note ooet oish \", '', \"'W colh y sad 't tuoed to tach ' said tlice. 's she waal s outh thiing to sind ter eas aft  SI dhall be aat nh d tor at aot. w dhppose  tu teang woewn d on ay dnn ahlrs  Ihet ihLL te a rueer woesgs wh se aore ' owever, tverything sn tuisr io -ey, \", '', \"'ust then she saadd ihmething woraceing about it ahe sror onlittle wiy of   tnd the waalewoar   oh sake out ohat tt was  tn tirst ahe whought tt wast be s loskes of oas ereret s  aut the  she wemembered oew sholl \", 'he was not  wnd the waon wake out ohat st was af y stlouse ooat sad bheteed tn aite ter elf  ', \"'Thuli st wecaf t y one  Io    sheught Alice. 'ah beeak th this eorse ' very hing ss tuoort -f -he -ay tou  aer   thet s whould bhing yory cike y an wom ahlk  tn tly rater whe e s no rand on thiing  \", \"Ah she segan  'IfcEuse  to you know wha foy oft of thes aoor 'T sn Iory lumed of thalp ng obout ier   tfcoute ' sAlice whought toes wost be she weght was tf tieaking ao s sorse  the wad nover sone ohch a cheng sefore  aut the wemembered oedeng them tn aer feekt r s aibtngTrymoen  aIncouse -'f cncorse,-'hum louse -'ncouse -'  ouse,'  she Mocse waoked at ter sether wn uiteoyoe y  and shemed to ber fa sit  oith tne of tt  aiktle sxes  wut st waid tot ing  \", \"'Weceaps yt woes 't bpder tand inglysh   sheughe tlice  't doye  l wn s allooath worse  tomi ofer tith thllicm whe santueror \", '', 'AAor  yhth t l ter seew yd e of tes ery, alice wad notrery luaaretoticn aew toog ssanand hing tod nedpened  ', \"'h she wegandonain. 'Ihlcatetaksaineer' shech sas the cirst whetence on aer fooeth Hiasons-ytk   he focse wote tnchceen wissetft of the soyer. and themed to buete  all tfer tith tiinhte\", '', \"Wh, w degayour farton ' tooed tlice aadtily  'nteid thet she wad narr bhi cror lndcal s aoet ng   'I wuite for et tou cod 'thtike tat   \", '', \"'Wot aike thts ' sried Ahe Mocse  an a lhaill  alrtion li ooice \", \"'Whuld bOU mite thts tt you dore wa ' \", \"'Ihll, tersaps yot ' said tlice.an a lhrd  ng oo e, 'Io 't be anyry dlout it 'Tnd tot i wosh t cauld nhau tou crt fonstinah  I shink you r bhle o coice oo tots on you dould nf y soe taa  'he wn tuch a loel ouittlahisgs  slice rant on  'olf to ter elf  an she waal woreny arout it ahe saor  aand the wad  auteieg to sote,y ay the tirs, aite ng oar hrrs and ras ing ter hoce,-and the tt auch a lete woot ahisg to soree,-'nd the s aurh a lotetil ofe oor tom h ng tone,-ar, t wegiyour fert n   sried Alice wsain, aor shes time the focse was seigtieeg all tfer  and the celt thrtainlat wast be aemdiy of  rded   Ih won't ghlk onout ier hnd rare tn you d beteer wot '\", '', \"'Thlcndeed ' sried the socse  who was shiabling aown ah the tnd of tes soml  'In tt y canld nhkk if toch a buclect 'Lfleaiciny aslays waT R cate  bett   soo  werirr toesgs  Ion't bot me taad whe seme osain.' \", \"'T dondt bng rd ' said tlice. 'n a lreat warry oo toange the sarbect wf  oucersation   Wne you -ane you cor' -tf -or tows ' she Mocse wod not cndwer  ah slice want on oateray  'Ihe e'ws ouch a lete dittle dowat drloft fause,t dhauld bike th baou tou  Tncittle seeght -ner wormeed  aou know. whlh tn, thrh aiog torie tyown terr 'And tt sl sieth thesgs aian tou chiou yhe   and tt sl sen op and defaaor at  aisce   and tll tamt  of chisgs -a han t memember tavf of thi  -and wt wegiwg  ao t sai er   ou know. wnd tersam  on s ao sse  l 'an s aand  t cardeee toosd  'Io shm, tt wnnl  cnl the tete ond -ar,wear   seeed Alice an a lhrtow ul wone. 'a m antaid t me gf erded tn wlain,' cor yhe socse was sialping anay toom ter hn sedd is st would no  and saneng tuite a conpinion on the door on st want  \", \"'h she womle  to tly anter at  aaacse toar! Io soue oeck inain. and tantondt bhlk inout iots af dons tather  it tou con't kike the  ' shon she rocse waadd thes  at worned tound and shal soewly ayck io ter  tt  tonk sas auite alrl aahth tarsion  alice whought   and tt wuid tn a lon raaabling woice  'ait as tot ih the thooe  and the  s ml ghll tou ci destiry, ynd tou rl bsder tand iha tt ws a havt wat  t d dona  \", '', \"'t was aash thme th so  aor the soor oas sotting tuite aoowd d aith the srtds ond dldcal  foat set toclen an o tt  the e ware nllucheand a lori   nsioy asd sldonrle   and sheeral wfher worious aoo rires, 'lice wot sha cos  and the coile sait  saat so she tooot  \", \"'\", '', '', 'HAPTER I..  AlCanses -ate and tlloug aure   he  were wn eed tnlueer took ng arrty ohet ss empled of the socy -the cetds woth tiew lad toer er   aha crcmal  oath the r sartohasgeng toase to the   and tll toanpetg tir  toass  and wndemeortaile ', '', 'The Mirst wuistion of course ihs  tow th tet toa wlain  the  wad n concire bion onouththis  and slter t lel tunute  tn wucmed tuite aorerel oo tlice wo tind her elf aolking aorenynrsy rath the   an st ahe wad nnow  ohe  w l tar site,', \"'t erd  the wad nuite srling rsrurent toth the sory  'ho wl oast rhrne  oocle  and sauld nn y soy  'I dn Ifd r that tou  wnd tost bnow wu ter s and then tlice wauld bot anl w ohth ut pnewing oow tnd tt was  tnd  as she Mory raottioe y replred to hhll tt  tlr  aha e was notsore th se aoyd  \", \"'l tast the Mocse  who waemed to be a farmon of tnthiriti olaue the   aolled out  aIom woen  wnl rn tour wnd tokten th ba 'T'ml mamn mane oou doe wlough ' she  tnl thy town as tnce  an a lorge wogei ahth the socse tn the sosdle  'lice weet her hyes andiously ooned an tt  aor the wolt thce the wauld nal h a lor oondeon the hid not cot ioa wory luon \", '', \"'T ,  ' \"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1001 1.30361\n",
      "1002 1.30001\n",
      "1003 1.22432\n",
      "1004 1.25952\n",
      "1005 1.22345\n",
      "1006 1.18432\n",
      "1007 1.24951\n",
      "1008 1.20153\n",
      "1009 1.20973\n",
      "1010 1.20903\n",
      "1011 1.16107\n",
      "1012 1.2409\n",
      "1013 1.2319\n",
      "1014 1.41674\n",
      "1015 1.30054\n",
      "1016 1.31512\n",
      "1017 1.29778\n",
      "1018 1.32234\n",
      "1019 1.24776\n",
      "1020 1.21391\n",
      "1021 1.23268\n",
      "1022 1.17532\n",
      "1023 1.23155\n",
      "1024 1.18047\n",
      "1025 1.20852\n",
      "1026 1.17386\n",
      "1027 1.17697\n",
      "1028 1.21582\n",
      "1029 1.22961\n",
      "1030 1.29545\n",
      "1031 1.35913\n",
      "1032 1.29045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-1383966c9445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss_val, _ = sess.run([mean_loss, updates],\n\u001b[0;32m---> 22\u001b[0;31m                            feed_dict={X: Xs, Y: Ys})\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "cursor = 0\n",
    "it_i = 0\n",
    "while True:\n",
    "    Xs, Ys = [], []\n",
    "    for batch_i in range(batch_size):\n",
    "        if (cursor + sequence_length) >= len(txt) - sequence_length - 1:\n",
    "            cursor = 0\n",
    "        Xs.append([encoder[ch]\n",
    "                   for ch in txt[cursor:cursor + sequence_length]])\n",
    "        Ys.append([encoder[ch]\n",
    "                   for ch in txt[cursor + 1: cursor + sequence_length + 1]])\n",
    "\n",
    "        cursor = (cursor + sequence_length)\n",
    "    Xs = np.array(Xs).astype(np.int32)\n",
    "    Ys = np.array(Ys).astype(np.int32)\n",
    "\n",
    "    loss_val, _ = sess.run([mean_loss, updates],\n",
    "                           feed_dict={X: Xs, Y: Ys})\n",
    "    print(it_i, loss_val)\n",
    "\n",
    "    if it_i % 500 == 0:\n",
    "        p = sess.run([Y_pred], feed_dict={X: Xs})[0]\n",
    "        preds = [decoder[p_i] for p_i in p]\n",
    "        print(\"\".join(preds).split('\\n'))\n",
    "\n",
    "    it_i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"extensions-1\"></a>\n",
    "## Extensions\n",
    "\n",
    "There are also certainly a lot of additions we can add to speed up or help with training including adding dropout or using batch normalization that I haven't gone into here.  Also when dealing with variable length sequences, you may want to consider using a special token to denote the last character or element in your sequence.\n",
    "\n",
    "As for applications, *completely endless*.  And I think that is really what makes this field so exciting right now.  There doesn't seem to be any limit to what is possible right now.  You are not just limited to text first of all.  You may want to feed in MIDI data to create a piece of algorithmic music.  I've tried it with raw sound data and this even works, but it requires a lot of memory and at least 30k iterations to run before it sounds like anything.  Or perhaps you might try some other unexpected text based information, such as encodings of image data like JPEG in base64.  Or other compressed data formats.  Or perhaps you are more adventurous and want to try using what you've learned here with the previous sessions to add recurrent layers to a traditional convolutional model.\n",
    "\n",
    "<a name=\"future\"></a>\n",
    "# Future\n",
    "\n",
    "If you're still here, then I'm really excited for you and to see what you'll create.  By now, you've seen most of the major building blocks with neural networks.  From here, you are only limited by the time it takes to train all of the interesting ideas you'll have.  But there is still so much more to discover, and it's very likely that this entire course is already out of date, because this field just moves incredibly fast.  In any case, the applications of these techniques are still fairly stagnant, so if you're here to see how your creative practice could grow with these techniques, then you should already have plenty to discover.\n",
    "\n",
    "I'm very excited about how the field is moving.  Often, it is very hard to find labels for a lot of data in a meaningful and consistent way.  But there is a lot of interesting stuff starting to emerge in the unsupervised models.  Those are the models that just take data in, and the computer reasons about it.  And even more interesting is the combination of general purpose learning algorithms.  That's really where reinforcement learning is starting to shine.  But that's for another course, perhaps.\n",
    "\n",
    "<a name=\"reading\"></a>\n",
    "# Reading\n",
    "\n",
    "Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Generative Adversarial Networks. 2014.\n",
    "https://arxiv.org/abs/1406.2661\n",
    "\n",
    "Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy.  Explaining and Harnessing Adversarial Examples.  2014.\n",
    "\n",
    "Alec Radford, Luke Metz, Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. 2015.\n",
    "https://arxiv.org/abs/1511.06434\n",
    "\n",
    "Emily Denton, Soumith Chintala, Arthur Szlam, Rob Fergus. \n",
    "Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks. 2015.\n",
    "arxiv.org/abs/1506.05751\n",
    "\n",
    "Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, Ole Winther. Autoencoding beyond pixels using a learned similarity metric. 2015.\n",
    "https://arxiv.org/abs/1512.09300\n",
    "\n",
    "Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro, Aaron Courville.  Adversarially Learned Inference.  2016.\n",
    "https://arxiv.org/abs/1606.00704\n",
    "\n",
    "Ilya Sutskever, James Martens, and Geoffrey Hinton. Generating Text with Recurrent Neural Networks, ICML 2011. \n",
    "\n",
    "A. Graves. Generating sequences with recurrent neural networks. In Arxiv preprint, arXiv:1308.0850, 2013.\n",
    "\n",
    "T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in\n",
    "Neural Information Processing Systems, pages 3111–3119, 2013.\n",
    "\n",
    "J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12, 2014.\n",
    "\n",
    "Yoon Kim, Yacine Jernite, David Sontag, Alexander M. Rush. Character-Aware Neural Language Models. 2015.\n",
    "https://arxiv.org/abs/1508.06615\n",
    "\n",
    "I. Sutskever, J. Martens, and G. Hinton. Generating text with recurrent neural networks. In L. Getoor and T. Scheffer, editors, Proceedings of the 28th International Conference on Machine Learning (ICML-11), ICML ’11, pages 1017–1024, New York, NY, USA, June 2011. ACM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
